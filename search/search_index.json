{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#_1","title":"\u6846\u67b6\u4ecb\u7ecd","text":"<p>\u4ee5\u4e0a\u662f\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5c06\u4e09\u7ef4\u6570\u636e\u6620\u5c04\u4e3a\u4e8c\u7ef4\uff0c\u7136\u540e\u5c06\u6240\u6709\u6570\u636e\u62fc\u63a5\u5728\u4e00\u8d77\uff0c\u518d\u8fdb\u884c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6240\u6709\u6570\u636e\u538b\u7f29\u4e3a\u4e8c\u7ef4\uff0c\u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\u540e\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u6d4b\u65f6\u95f4\u7684 MLP \u76f4\u63a5\u8f93\u5165\u5230 Transformer \u4e2d\uff0c\u6700\u7ec8\u8fdb\u5165 Predict \u5c42\u8ba1\u7b97\u6240\u6709\u8f93\u5165\u53d8\u91cf\u7684\u9884\u6d4b\u503c\u3002\u6700\u540e\u6839\u636e\u9884\u6d4b\u7684\u65f6\u95f4\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u6574\u4e2a\u6d41\u7a0b\u7b80\u5355\u800c\u76f4\u63a5\u3002</p> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u6a21\u578b\u4f1a\u968f\u673a\u751f\u6210\u4e00\u4e2a\u9884\u6d4b\u65f6\u95f4 \u2206\ud835\udc61\uff0c\u2206\ud835\udc61 \u223c \ud835\udcb0[6, 168]\uff0c\u7136\u540e\u6839\u636e\u6b64\u53d8\u91cf\u8ba1\u7b97\u7ed3\u679c\u548c\u635f\u5931\u3002</p>"},{"location":"architecture/#_2","title":"\u635f\u5931\u51fd\u6570","text":"<p>\u635f\u5931\u51fd\u6570\u4f7f\u7528\u52a0\u6743\u5747\u65b9\u8bef\u5dee\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5176\u4e2d\u52a0\u6743\u5e73\u5747\u503c\u5f88\u76f4\u89c2\u3002</p>"},{"location":"architecture/#finetuning","title":"Finetuning","text":"<p>\u5bf9\u4e8e\u4f7f\u7528\u4ee5\u4e0a\u65b9\u6cd5\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5fae\u8c03\u5f88\u7b80\u5355\uff0c\u5206\u4e3a\u4e24\u79cd\u60c5\u51b5\u3002\u7b2c\u4e00\u79cd\u60c5\u51b5\u662f\uff0c\u5982\u679c\u9884\u8bad\u7ec3\u96c6\u5305\u62ec\u8be5\u5143\u7d20\uff0c\u5219\u53ef\u4ee5\u76f4\u63a5\u5728\u8bad\u7ec3\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002\u7b2c\u4e8c\u79cd\u60c5\u51b5\u662f\uff0c\u9700\u8981\u66f4\u6362 Header \u548c Prediction\uff0c\u7136\u540e\u51b3\u5b9a\u662f\u5426\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd8\u53ef\u4ee5\u6dfb\u52a0\u4e00\u4e2a Neck \u5c42\u6765\u9002\u5e94\u5386\u53f2\u6570\u636e\u3002</p>"},{"location":"architecture/#climax.arch.ClimaX","title":"<code>ClimaX</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Implements the ClimaX model as described in the paper, https://arxiv.org/abs/2301.10343</p> <p>Parameters:</p> Name Type Description Default <code>default_vars</code> <code>list</code> <p>list of default variables to be used for training</p> required <code>img_size</code> <code>list</code> <p>image size of the input data</p> <code>[32, 64]</code> <code>patch_size</code> <code>int</code> <p>patch size of the input data</p> <code>2</code> <code>embed_dim</code> <code>int</code> <p>embedding dimension</p> <code>1024</code> <code>depth</code> <code>int</code> <p>number of transformer layers</p> <code>8</code> <code>decoder_depth</code> <code>int</code> <p>number of decoder layers</p> <code>2</code> <code>num_heads</code> <code>int</code> <p>number of attention heads</p> <code>16</code> <code>mlp_ratio</code> <code>float</code> <p>ratio of mlp hidden dimension to embedding dimension</p> <code>4.0</code> <code>drop_path</code> <code>float</code> <p>stochastic depth rate</p> <code>0.1</code> <code>drop_rate</code> <code>float</code> <p>dropout rate</p> <code>0.1</code> <code>parallel_patch_embed</code> <code>bool</code> <p>whether to use parallel patch embedding</p> <code>True</code> Source code in <code>src/climax/arch.py</code> <pre><code>class ClimaX(nn.Module):\n\"\"\"Implements the ClimaX model as described in the paper,\n    https://arxiv.org/abs/2301.10343\n    Args:\n        default_vars (list): list of default variables to be used for training\n        img_size (list): image size of the input data\n        patch_size (int): patch size of the input data\n        embed_dim (int): embedding dimension\n        depth (int): number of transformer layers\n        decoder_depth (int): number of decoder layers\n        num_heads (int): number of attention heads\n        mlp_ratio (float): ratio of mlp hidden dimension to embedding dimension\n        drop_path (float): stochastic depth rate\n        drop_rate (float): dropout rate\n        parallel_patch_embed (bool): whether to use parallel patch embedding\n    \"\"\"\ndef __init__(\nself,\ndefault_vars,\nimg_size=[32, 64],\npatch_size=2,\nembed_dim=1024,\ndepth=8,\ndecoder_depth=2,\nnum_heads=16,\nmlp_ratio=4.0,\ndrop_path=0.1,\ndrop_rate=0.1,\nparallel_patch_embed=True,\n):\nsuper().__init__()\n# TODO: remove time_history parameter\nself.img_size = img_size\nself.patch_size = patch_size\nself.default_vars = default_vars\nself.parallel_patch_embed = parallel_patch_embed\n# variable tokenization: separate embedding layer for each input variable\nif self.parallel_patch_embed:\nself.token_embeds = ParallelVarPatchEmbed(len(default_vars), img_size, patch_size, embed_dim)\nself.num_patches = self.token_embeds.num_patches\nelse:\nself.token_embeds = nn.ModuleList(\n[PatchEmbed(img_size, patch_size, 1, embed_dim) for i in range(len(default_vars))]\n)\nself.num_patches = self.token_embeds[0].num_patches\n# variable embedding to denote which variable each token belongs to\n# helps in aggregating variables\nself.var_embed, self.var_map = self.create_var_embedding(embed_dim)\n# variable aggregation: a learnable query and a single-layer cross attention\nself.var_query = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True) # type: ignore\nself.var_agg = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n# positional embedding and lead time embedding\nself.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim), requires_grad=True) # type: ignore\nself.lead_time_embed = nn.Linear(1, embed_dim)\n# --------------------------------------------------------------------------\n# ViT backbone\nself.pos_drop = nn.Dropout(p=drop_rate)\ndpr = [x.item() for x in torch.linspace(0, drop_path, depth)]  # stochastic depth decay rule\nself.blocks = nn.ModuleList(\n[\nBlock(\nembed_dim,\nnum_heads,\nmlp_ratio,\nqkv_bias=True,\ndrop_path=dpr[i],\nnorm_layer=nn.LayerNorm,\ndrop=drop_rate,\n)\nfor i in range(depth)\n]\n)\nself.norm = nn.LayerNorm(embed_dim)\n# --------------------------------------------------------------------------\n# prediction head\nself.head = nn.ModuleList()\nfor _ in range(decoder_depth):\nself.head.append(nn.Linear(embed_dim, embed_dim))\nself.head.append(nn.GELU())\nself.head.append(nn.Linear(embed_dim, len(self.default_vars) * patch_size**2))\nself.head = nn.Sequential(*self.head)\n# --------------------------------------------------------------------------\nself.initialize_weights()\ndef initialize_weights(self):\n# initialize pos_emb and var_emb\npos_embed = get_2d_sincos_pos_embed(\nself.pos_embed.shape[-1],\nint(self.img_size[0] / self.patch_size),\nint(self.img_size[1] / self.patch_size),\ncls_token=False,\n)\nself.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\nvar_embed = get_1d_sincos_pos_embed_from_grid(self.var_embed.shape[-1], np.arange(len(self.default_vars)))\nself.var_embed.data.copy_(torch.from_numpy(var_embed).float().unsqueeze(0))\n# token embedding layer\nif self.parallel_patch_embed:\nfor i in range(len(self.token_embeds.proj_weights)): # type: ignore\nw = self.token_embeds.proj_weights[i].data # type: ignore\ntrunc_normal_(w.view([w.shape[0], -1]), std=0.02)\nelse:\nfor i in range(len(self.token_embeds)): # type: ignore\nw = self.token_embeds[i].proj.weight.data # type: ignore\ntrunc_normal_(w.view([w.shape[0], -1]), std=0.02) # type: ignore\n# initialize nn.Linear and nn.LayerNorm\nself.apply(self._init_weights)\ndef _init_weights(self, m):\nif isinstance(m, nn.Linear):\ntrunc_normal_(m.weight, std=0.02)\nif m.bias is not None:\nnn.init.constant_(m.bias, 0)\nelif isinstance(m, nn.LayerNorm):\nnn.init.constant_(m.bias, 0)\nnn.init.constant_(m.weight, 1.0)\ndef create_var_embedding(self, dim):\nvar_embed = nn.Parameter(torch.zeros(1, len(self.default_vars), dim), requires_grad=True) # type: ignore\n# TODO: create a mapping from var --&gt; idx\nvar_map = {}\nidx = 0\nfor var in self.default_vars:\nvar_map[var] = idx\nidx += 1\nreturn var_embed, var_map\n@lru_cache(maxsize=None)\ndef get_var_ids(self, vars, device):\nids = np.array([self.var_map[var] for var in vars])\nreturn torch.from_numpy(ids).to(device)\ndef get_var_emb(self, var_emb, vars):\nids = self.get_var_ids(vars, var_emb.device)\nreturn var_emb[:, ids, :]\ndef unpatchify(self, x: torch.Tensor, h=None, w=None):\n\"\"\"\n        x: (B, L, V * patch_size**2)\n        return imgs: (B, V, H, W)\n        \"\"\"\np = self.patch_size\nc = len(self.default_vars)\nh = self.img_size[0] // p if h is None else h // p\nw = self.img_size[1] // p if w is None else w // p\nassert h * w == x.shape[1]\nx = x.reshape(shape=(x.shape[0], h, w, p, p, c))\nx = torch.einsum(\"nhwpqc-&gt;nchpwq\", x)\nimgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\nreturn imgs\ndef aggregate_variables(self, x: torch.Tensor):\n\"\"\"\n        x: B, V, L, D\n        \"\"\"\nb, _, l, _ = x.shape\nx = torch.einsum(\"bvld-&gt;blvd\", x)\nx = x.flatten(0, 1)  # BxL, V, D\nvar_query = self.var_query.repeat_interleave(x.shape[0], dim=0)\nx, _ = self.var_agg(var_query, x, x)  # BxL, D\nx = x.squeeze()\nx = x.unflatten(dim=0, sizes=(b, l))  # B, L, D\nreturn x\ndef forward_encoder(self, x: torch.Tensor, lead_times: torch.Tensor, variables):\n# x: `[B, V, H, W]` shape.\nif isinstance(variables, list):\nvariables = tuple(variables)\n# tokenize each variable separately\nembeds = []\nvar_ids = self.get_var_ids(variables, x.device)\nif self.parallel_patch_embed:\nx = self.token_embeds(x, var_ids)  # B, V, L, D\nelse:\nfor i in range(len(var_ids)):\nid = var_ids[i]\nembeds.append(self.token_embeds[id](x[:, i : i + 1])) # type: ignore\nx = torch.stack(embeds, dim=1)  # B, V, L, D\n# add variable embedding\nvar_embed = self.get_var_emb(self.var_embed, variables)\nx = x + var_embed.unsqueeze(2)  # B, V, L, D\n# variable aggregation\nx = self.aggregate_variables(x)  # B, L, D\n# add pos embedding\nx = x + self.pos_embed\n# add lead time embedding\nlead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1))  # B, D\nlead_time_emb = lead_time_emb.unsqueeze(1)\nx = x + lead_time_emb  # B, L, D\nx = self.pos_drop(x)\n# apply Transformer blocks\nfor blk in self.blocks:\nx = blk(x)\nx = self.norm(x)\nreturn x\ndef forward(self, x, y, lead_times, variables, out_variables, metric, lat):\n\"\"\"Forward pass through the model.\n        Args:\n            x: `[B, Vi, H, W]` shape. Input weather/climate variables\n            y: `[B, Vo, H, W]` shape. Target weather/climate variables\n            lead_times: `[B]` shape. Forecasting lead times of each element of the batch.\n        Returns:\n            loss (list): Different metrics.\n            preds (torch.Tensor): `[B, Vo, H, W]` shape. Predicted weather/climate variables.\n        \"\"\"\nout_transformers = self.forward_encoder(x, lead_times, variables)  # B, L, D\npreds = self.head(out_transformers)  # B, L, V*p*p\npreds = self.unpatchify(preds)\nout_var_ids = self.get_var_ids(tuple(out_variables), preds.device)\npreds = preds[:, out_var_ids]\nif metric is None:\nloss = None\nelse:\nloss = [m(preds, y, out_variables, lat) for m in metric]\nreturn loss, preds\ndef evaluate(self, x, y, lead_times, variables, out_variables, transform, metrics, lat, clim, log_postfix):\n_, preds = self.forward(x, y, lead_times, variables, out_variables, metric=None, lat=lat)\nreturn [m(preds, y, transform, out_variables, lat, clim, log_postfix) for m in metrics]\n</code></pre>"},{"location":"architecture/#climax.arch.ClimaX.aggregate_variables","title":"<code>aggregate_variables(x)</code>","text":"<p>x: B, V, L, D</p> Source code in <code>src/climax/arch.py</code> <pre><code>def aggregate_variables(self, x: torch.Tensor):\n\"\"\"\n    x: B, V, L, D\n    \"\"\"\nb, _, l, _ = x.shape\nx = torch.einsum(\"bvld-&gt;blvd\", x)\nx = x.flatten(0, 1)  # BxL, V, D\nvar_query = self.var_query.repeat_interleave(x.shape[0], dim=0)\nx, _ = self.var_agg(var_query, x, x)  # BxL, D\nx = x.squeeze()\nx = x.unflatten(dim=0, sizes=(b, l))  # B, L, D\nreturn x\n</code></pre>"},{"location":"architecture/#climax.arch.ClimaX.forward","title":"<code>forward(x, y, lead_times, variables, out_variables, metric, lat)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p><code>[B, Vi, H, W]</code> shape. Input weather/climate variables</p> required <code>y</code> <p><code>[B, Vo, H, W]</code> shape. Target weather/climate variables</p> required <code>lead_times</code> <p><code>[B]</code> shape. Forecasting lead times of each element of the batch.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>list</code> <p>Different metrics.</p> <code>preds</code> <code>torch.Tensor</code> <p><code>[B, Vo, H, W]</code> shape. Predicted weather/climate variables.</p> Source code in <code>src/climax/arch.py</code> <pre><code>def forward(self, x, y, lead_times, variables, out_variables, metric, lat):\n\"\"\"Forward pass through the model.\n    Args:\n        x: `[B, Vi, H, W]` shape. Input weather/climate variables\n        y: `[B, Vo, H, W]` shape. Target weather/climate variables\n        lead_times: `[B]` shape. Forecasting lead times of each element of the batch.\n    Returns:\n        loss (list): Different metrics.\n        preds (torch.Tensor): `[B, Vo, H, W]` shape. Predicted weather/climate variables.\n    \"\"\"\nout_transformers = self.forward_encoder(x, lead_times, variables)  # B, L, D\npreds = self.head(out_transformers)  # B, L, V*p*p\npreds = self.unpatchify(preds)\nout_var_ids = self.get_var_ids(tuple(out_variables), preds.device)\npreds = preds[:, out_var_ids]\nif metric is None:\nloss = None\nelse:\nloss = [m(preds, y, out_variables, lat) for m in metric]\nreturn loss, preds\n</code></pre>"},{"location":"architecture/#climax.arch.ClimaX.unpatchify","title":"<code>unpatchify(x, h=None, w=None)</code>","text":"<p>x: (B, L, V * patch_size**2) return imgs: (B, V, H, W)</p> Source code in <code>src/climax/arch.py</code> <pre><code>def unpatchify(self, x: torch.Tensor, h=None, w=None):\n\"\"\"\n    x: (B, L, V * patch_size**2)\n    return imgs: (B, V, H, W)\n    \"\"\"\np = self.patch_size\nc = len(self.default_vars)\nh = self.img_size[0] // p if h is None else h // p\nw = self.img_size[1] // p if w is None else w // p\nassert h * w == x.shape[1]\nx = x.reshape(shape=(x.shape[0], h, w, p, p, c))\nx = torch.einsum(\"nhwpqc-&gt;nchpwq\", x)\nimgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\nreturn imgs\n</code></pre>"},{"location":"discussion/","title":"Discussion","text":""},{"location":"discussion/#discussion","title":"Discussion","text":"<p>\u5bf9\u4e8e\u8fd9\u4e2a\u6846\u67b6\uff0c\u6216\u8bb8\u53ef\u4ee5\u518d\u4ee5\u4e0b\u65b9\u9762\u8fdb\u884c\u6539\u8fdb\u3002</p> <ol> <li> <p>\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u591a\u5f20\u56fe\u6765\u5c55\u793a\u4e00\u4e2a\u533a\u57df\u7684\u6570\u636e\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u4e86\u89e3\u5176\u6c14\u5019\u60c5\u51b5\u3002</p> </li> <li> <p>\u6211\u4eec\u53ef\u4ee5\u6dfb\u52a0\u4e00\u4e2a\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5728\u6b64\u9636\u6bb5\u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u76d6\u7684\u6570\u636e\u6765\u66f4\u597d\u5730\u7406\u89e3\u6c14\u5019\u77e5\u8bc6\u3002\u8fd9\u6837\uff0c\u5728\u8fdb\u884c\u9884\u6d4b\u4e4b\u524d\uff0c\u6211\u4eec\u5c31\u80fd\u66f4\u597d\u5730\u4e86\u89e3\u6c14\u5019\u7684\u57fa\u672c\u60c5\u51b5\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u975e\u5e38\u91cd\u8981\u3002</p> </li> </ol>"},{"location":"evaluate/","title":"\u8bc4\u4f30\u4ee3\u7801","text":""},{"location":"install/","title":"Installation Guide","text":"clone the repo<pre><code>git clone https://github.com/microsoft/ClimaX\n</code></pre> <code>conda</code><code>docker</code> create and activate env<pre><code>cd ClimaX\nconda env create --file docker/environment.yml\nconda activate climaX\n</code></pre> install this package<pre><code># install so the project is in PYTHONPATH\npip install -e .\n</code></pre> build docker container<pre><code>cd docker\ndocker build -t ClimaX .\n</code></pre> run docker container<pre><code>cd ClimaX\ndocker run --gpus all -it --rm --user $(id -u):$(id -g) \\\n-v $(pwd):/code -v /mnt/data:/data --workdir /code -e PYTHONPATH=/code/src \\\nClimaX:latest\n</code></pre> <p>Note</p> <ul> <li><code>--gpus all -it --rm --user $(id -u):$(id -g)</code>: enables using all GPUs and runs an interactive session with current user's UID/GUID to avoid <code>docker</code> writing files as root.</li> <li><code>-v $(pwd):/code -v /mnt/data:/data --workdir /code</code>: mounts current directory and data directory (i.e. the cloned git repo) to <code>/code</code> and <code>/data</code> respectively, and use the <code>code</code> directory as the current working directory.</li> </ul>"},{"location":"preprocess/","title":"Preprocess","text":""},{"location":"preprocess/#data_preprocessing.nc2np_equally_era5.main","title":"<code>main(root_dir, save_dir, variables, start_train_year, start_val_year, start_test_year, end_year, num_shards)</code>","text":"<p>\u9884\u5904\u7406\u4e3b\u51fd\u6570</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>string</code> <p>\u6570\u636e\u6839\u76ee\u5f55</p> required <code>save_dir</code> <code>string</code> <p>\u6570\u636e\u5904\u7406\u540e\u7684\u4fdd\u5b58\u76ee\u5f55</p> required <code>variables</code> <code>string</code> <p>\u53d8\u91cf\u540d</p> required <code>start_train_year</code> <code>int</code> <p>\u8bad\u7ec3\u6570\u636e\u8d77\u59cb\u5e74\u4efd</p> required <code>start_val_year</code> <code>int</code> <p>\u9a8c\u8bc1\u6570\u636e\u8d77\u59cb\u5e74\u4efd</p> required <code>start_test_year</code> <code>int</code> <p>\u6d4b\u8bd5\u6570\u636e\u8d77\u59cb\u5e74\u4efd</p> required <code>end_year</code> <code>int</code> <p>\u6570\u636e\u622a\u6b62\u5e74\u4efd</p> required <code>num_shards</code> <code>int</code> <p>\u6570\u636e\u5207\u7247\u6570</p> required Source code in <code>src/data_preprocessing/nc2np_equally_era5.py</code> <pre><code>@click.command()\n@click.option(\"--root_dir\", type=click.Path(exists=True)) # \u6570\u636e\u6839\u76ee\u5f55\n@click.option(\"--save_dir\", type=str) # \u6570\u636e\u5904\u7406\u540e\u7684\u4fdd\u5b58\u76ee\u5f55\n@click.option(\n\"--variables\", # \u53d8\u91cf\u540d\n\"-v\",\ntype=click.STRING,\nmultiple=True,\ndefault=[\n\"2m_temperature\",\n\"10m_u_component_of_wind\",\n\"10m_v_component_of_wind\",\n\"toa_incident_solar_radiation\",\n\"total_precipitation\",\n\"geopotential\",\n\"u_component_of_wind\",\n\"v_component_of_wind\",\n\"temperature\",\n\"relative_humidity\",\n\"specific_humidity\",\n],\n)\n@click.option(\"--start_train_year\", type=int, default=1979) # \u8bad\u7ec3\u6570\u636e\u8d77\u59cb\u5e74\u4efd\n@click.option(\"--start_val_year\", type=int, default=2016) # \u9a8c\u8bc1\u6570\u636e\u8d77\u59cb\u5e74\u4efd\n@click.option(\"--start_test_year\", type=int, default=2017) # \u6d4b\u8bd5\u6570\u636e\u8d77\u59cb\u5e74\u4efd\n@click.option(\"--end_year\", type=int, default=2019) # \u6570\u636e\u622a\u6b62\u5e74\u4efd\n@click.option(\"--num_shards\", type=int, default=8) # \u6570\u636e\u5207\u7247\u6570\ndef main(\nroot_dir,\nsave_dir,\nvariables,\nstart_train_year,\nstart_val_year,\nstart_test_year,\nend_year,\nnum_shards,\n):\n'''\n    \u9884\u5904\u7406\u4e3b\u51fd\u6570\n    Args:\n        root_dir (string): \u6570\u636e\u6839\u76ee\u5f55\n        save_dir (string): \u6570\u636e\u5904\u7406\u540e\u7684\u4fdd\u5b58\u76ee\u5f55\n        variables (string): \u53d8\u91cf\u540d\n        start_train_year (int): \u8bad\u7ec3\u6570\u636e\u8d77\u59cb\u5e74\u4efd\n        start_val_year (int): \u9a8c\u8bc1\u6570\u636e\u8d77\u59cb\u5e74\u4efd\n        start_test_year (int): \u6d4b\u8bd5\u6570\u636e\u8d77\u59cb\u5e74\u4efd\n        end_year (int): \u6570\u636e\u622a\u6b62\u5e74\u4efd\n        num_shards (int): \u6570\u636e\u5207\u7247\u6570\n    '''\nassert start_val_year &gt; start_train_year and start_test_year &gt; start_val_year and end_year &gt; start_test_year\ntrain_years = range(start_train_year, start_val_year) # \u8bad\u7ec3\u5e74\u4efd\u5217\u8868\nval_years = range(start_val_year, start_test_year) # \u9a8c\u8bc1\u5e74\u4efd\u5217\u8868\ntest_years = range(start_test_year, end_year) # \u6d4b\u8bd5\u5e74\u4efd\u5217\u8868\nos.makedirs(save_dir, exist_ok=True) # \u5982\u679c\u4fdd\u5b58\u76ee\u5f55\u4e0d\u5b58\u5728\u5219\u521b\u5efa\nnc2np(root_dir, variables, train_years, save_dir, \"train\", num_shards) # \u8c03\u7528nc2np\u51fd\u6570\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u6362\u4e3anumpy\u683c\u5f0f\uff0c\u5e76\u4fdd\u5b58\u4e3anpz\u6587\u4ef6\nnc2np(root_dir, variables, val_years, save_dir, \"val\", num_shards) \nnc2np(root_dir, variables, test_years, save_dir, \"test\", num_shards)\n# save lat and lon data \u4fdd\u5b58\u7ecf\u7eac\u5ea6\u6570\u636e\nps = glob.glob(os.path.join(root_dir, variables[0], f\"*{train_years[0]}*.nc\")) # \u83b7\u53d6\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u5e74\u4efd\u7684nc\u6587\u4ef6\u8def\u5f84\nx = xr.open_mfdataset(ps[0], parallel=True) # \u6253\u5f00nc\u6587\u4ef6\nlat = x[\"lat\"].to_numpy() # \u83b7\u53d6\u7eac\u5ea6\u6570\u7ec4\nlon = x[\"lon\"].to_numpy() # \u83b7\u53d6\u7ecf\u5ea6\u6570\u7ec4\nnp.save(os.path.join(save_dir, \"lat.npy\"), lat) # \u4fdd\u5b58\u7eac\u5ea6\u6570\u7ec4\nnp.save(os.path.join(save_dir, \"lon.npy\"), lon) # \u4fdd\u5b58\u7ecf\u5ea6\u6570\u7ec4\n</code></pre>"},{"location":"preprocess/#data_preprocessing.nc2np_equally_era5.nc2np","title":"<code>nc2np(path, variables, years, save_dir, partition, num_shards_per_year)</code>","text":"<p>\u5c06ERA5\u8d44\u6599\u8f6c\u6362\u4e3anumpy\u6570\u7ec4, \u5e76\u5c06\u5176\u5b58\u50a8\u4e3a.npy\u6587\u4ef6\u3002</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>string</code> <p>\u8d44\u6599\u6240\u5728\u8def\u5f84</p> required <code>variables</code> <code>list</code> <p>\u53d8\u91cf\u540d\u5217\u8868</p> required <code>years</code> <code>list</code> <p>\u5e74\u4efd\u5217\u8868</p> required <code>save_dir</code> <code>string</code> <p>\u5b58\u50a8\u8def\u5f84</p> required <code>partition</code> <code>string</code> <p>\"train\" \u6216 \"test\", \u6570\u636e\u96c6\u7c7b\u578b</p> required <code>num_shards_per_year</code> <code>int</code> <p>\u6bcf\u5e74\u7684\u5206\u7247\u6570</p> required Source code in <code>src/data_preprocessing/nc2np_equally_era5.py</code> <pre><code>def nc2np(path, variables, years, save_dir, partition, num_shards_per_year):\n'''\n    \u5c06ERA5\u8d44\u6599\u8f6c\u6362\u4e3anumpy\u6570\u7ec4, \u5e76\u5c06\u5176\u5b58\u50a8\u4e3a.npy\u6587\u4ef6\u3002\n    Args:\n        path (string): \u8d44\u6599\u6240\u5728\u8def\u5f84\n        variables (list): \u53d8\u91cf\u540d\u5217\u8868\n        years (list): \u5e74\u4efd\u5217\u8868\n        save_dir (string): \u5b58\u50a8\u8def\u5f84\n        partition (string): \"train\" \u6216 \"test\", \u6570\u636e\u96c6\u7c7b\u578b\n        num_shards_per_year (int): \u6bcf\u5e74\u7684\u5206\u7247\u6570\n    '''\n# \u521b\u5efa\u76ee\u5f55\nos.makedirs(os.path.join(save_dir, partition), exist_ok=True)\nif partition == \"train\": # \u5982\u679c\u4e3a\u8bad\u7ec3\u96c6\nnormalize_mean = {} # \u521b\u5efa\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u653e\u5f52\u4e00\u5316\u540e\u7684\u5747\u503c\nnormalize_std = {} # \u521b\u5efa\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u653e\u5f52\u4e00\u5316\u540e\u7684\u6807\u51c6\u5dee\nclimatology = {} # \u521b\u5efa\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u653e\u6c14\u5019\u8d44\u6599\nconstants = xr.open_mfdataset(os.path.join(path, \"constants.nc\"), combine=\"by_coords\", parallel=True) # \u6253\u5f00\u5e38\u91cf\u8d44\u6599\nconstant_fields = [\"land_sea_mask\", \"orography\", \"lattitude\"] # \u5e38\u91cf\u53d8\u91cf\u540d\nconstant_values = {} # \u521b\u5efa\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u653e\u5e38\u91cf\u8d44\u6599\nfor f in constant_fields:\nconstant_values[f] = np.expand_dims(constants[NAME_TO_VAR[f]].to_numpy(), axis=(0, 1)).repeat(HOURS_PER_YEAR, axis=0) # \u83b7\u53d6\u5e38\u91cf\u503c\u5e76\u5c06\u7ef4\u5ea6\u6269\u5c55\nif partition == \"train\": # \u5982\u679c\u4e3a\u8bad\u7ec3\u96c6\nnormalize_mean[f] = constant_values[f].mean(axis=(0, 2, 3)) # \u8ba1\u7b97\u5747\u503c\u5e76\u5b58\u50a8\nnormalize_std[f] = constant_values[f].std(axis=(0, 2, 3)) # \u8ba1\u7b97\u6807\u51c6\u5dee\u5e76\u5b58\u50a8\nfor year in tqdm(years): # \u8fed\u4ee3\u6bcf\u4e00\u5e74\u4efd\nnp_vars = {} # \u521b\u5efa\u7a7a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u653enumpy\u6570\u7ec4\n# \u5e38\u91cf\u53d8\u91cf\nfor f in constant_fields:\nnp_vars[f] = constant_values[f]\n# \u975e\u5e38\u91cf\u53d8\u91cf\nfor var in variables:\nps = glob.glob(os.path.join(path, var, f\"*{year}*.nc\")) # \u67e5\u627e\u53d8\u91cf\u6240\u5bf9\u5e94\u7684.nc\u6587\u4ef6\u8def\u5f84\nds = xr.open_mfdataset(ps, combine=\"by_coords\", parallel=True)  # \u901a\u8fc7xarray\u6253\u5f00.nc\u6587\u4ef6\uff0c\u5408\u5e76\u540c\u540d\u5750\u6807\ncode = NAME_TO_VAR[var] # \u67e5\u627e\u53d8\u91cf\u540d\u6240\u5bf9\u5e94\u7684\u7f16\u7801\nif len(ds[code].shape) == 3:  # \u8868\u5c42\u53d8\u91cf\nds[code] = ds[code].expand_dims(\"val\", axis=1) # \u7ed9\u53d8\u91cf\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\u4e3a\"val\"\n# remove the last 24 hours if this year has 366 days\nnp_vars[var] = ds[code].to_numpy()[:HOURS_PER_YEAR] # \u83b7\u53d6\u53d8\u91cfvar\u7684\u503c\uff0c\u53d6\u4e00\u5e74\u7684\u503c\nif partition == \"train\":  # compute mean and std of each var in each year\nvar_mean_yearly = np_vars[var].mean(axis=(0, 2, 3)) # \u8ba1\u7b97\u6bcf\u4e2a\u5e74\u4efdvar\u7684\u5747\u503c\nvar_std_yearly = np_vars[var].std(axis=(0, 2, 3)) # \u8ba1\u7b97\u6bcf\u4e2a\u5e74\u4efdvar\u7684\u6807\u51c6\u5dee\nif var not in normalize_mean:\nnormalize_mean[var] = [var_mean_yearly] # \u7b2c\u4e00\u6b21\u5904\u7406\u8be5\u53d8\u91cf\uff0c\u521d\u59cb\u5316\u5747\u503c\u6570\u7ec4\nnormalize_std[var] = [var_std_yearly] # \u7b2c\u4e00\u6b21\u5904\u7406\u8be5\u53d8\u91cf\uff0c\u521d\u59cb\u5316\u6807\u51c6\u5dee\u6570\u7ec4\nelse:\nnormalize_mean[var].append(var_mean_yearly) # \u5c06\u8be5\u5e74\u4efd\u7684\u5747\u503c\u6dfb\u52a0\u5230\u5747\u503c\u6570\u7ec4\nnormalize_std[var].append(var_std_yearly) # \u5c06\u8be5\u5e74\u4efd\u7684\u6807\u51c6\u5dee\u6dfb\u52a0\u5230\u6807\u51c6\u5dee\u6570\u7ec4\nclim_yearly = np_vars[var].mean(axis=0) # \u8ba1\u7b97\u4e00\u5e74\u4e2d\u53d8\u91cfvar\u7684\u5e73\u5747\u503c\nif var not in climatology:\nclimatology[var] = [clim_yearly] # \u7b2c\u4e00\u6b21\u5904\u7406\u8be5\u53d8\u91cf\uff0c\u521d\u59cb\u5316climatology\nelse:\nclimatology[var].append(clim_yearly) # \u5c06\u8be5\u5e74\u4efd\u7684climatology\u6dfb\u52a0\u5230\u6570\u7ec4\nelse:  # \u591a\u5c42\u53d8\u91cf\uff0c\u53ea\u4f7f\u7528\u90e8\u5206\u5c42\nassert len(ds[code].shape) == 4 # \u65ad\u8a00\u8be5\u53d8\u91cf\u7684\u5f62\u72b6\u662f4\u7ef4\nall_levels = ds[\"level\"][:].to_numpy() # \u83b7\u53d6\u8be5\u53d8\u91cf\u6240\u6709\u7684\u5c42\nall_levels = np.intersect1d(all_levels, DEFAULT_PRESSURE_LEVELS) # \u53d6\u6240\u6709\u5c42\u4e0e\u9ed8\u8ba4\u538b\u529b\u5c42\u7684\u4ea4\u96c6\nfor level in all_levels:\nds_level = ds.sel(level=[level]) # \u53d6\u51fa\u8be5\u53d8\u91cf\u5728\u8be5\u5c42\u7684\u503c\nlevel = int(level)\n# remove the last 24 hours if this year has 366 days\nnp_vars[f\"{var}_{level}\"] = ds_level[code].to_numpy()[:HOURS_PER_YEAR] # \u83b7\u53d6\u8be5\u53d8\u91cf\u5728\u8be5\u5c42\u7684\u503c\uff0c\u53d6\u4e00\u5e74\u7684\u503c\nif partition == \"train\":  # \u8ba1\u7b97\u6bcf\u5e74\u6bcf\u4e2a\u53d8\u91cf\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\nvar_mean_yearly = np_vars[f\"{var}_{level}\"].mean(axis=(0, 2, 3)) # \u8ba1\u7b97\u6bcf\u4e2a\u5e74\u4efd\u8be5\u53d8\u91cf\u5728\u8be5\u5c42\u7684\u5747\u503c\nvar_std_yearly = np_vars[f\"{var}_{level}\"].std(axis=(0, 2, 3)) # \u8ba1\u7b97\u6bcf\u4e2a\u5e74\u4efd\u8be5\u53d8\u91cf\u5728\u8be5\u5c42\u7684\u6807\u51c6\u5dee\nif var not in normalize_mean:\nnormalize_mean[f\"{var}_{level}\"] = [var_mean_yearly] # \u7b2c\u4e00\u6b21\u5904\u7406\u8be5\u53d8\u91cf\uff0c\u521d\u59cb\u5316\u5747\u503c\u6570\u7ec4\nnormalize_std[f\"{var}_{level}\"] = [var_std_yearly] # \u7b2c\u4e00\u6b21\u5904\u7406\u8be5\u53d8\u91cf\uff0c\u521d\u59cb\u5316\u6807\u51c6\u5dee\u6570\u7ec4\nelse:\nnormalize_mean[f\"{var}_{level}\"].append(var_mean_yearly) # \u5c06\u8be5\u5e74\u4efd\u8be5\u5c42\u7684\u5747\u503c\u6dfb\u52a0\u5230\u5747\u503c\u6570\u7ec4\nnormalize_std[f\"{var}_{level}\"].append(var_std_yearly) # \u5c06\u8be5\u5e74\u4efd\u8be5\u5c42\u7684\u6807\u51c6\u5dee\u6dfb\u52a0\u5230\u6807\u51c6\u5dee\u6570\u7ec4\nclim_yearly = np_vars[f\"{var}_{level}\"].mean(axis=0) # \u8ba1\u7b97\u4e00\u5e74\u4e2d\u53d8\u91cf\u5728\u8be5\u5c42\u7684\u5e73\u5747\u503c\nif f\"{var}_{level}\" not in climatology:\nclimatology[f\"{var}_{level}\"] = [clim_yearly] # \u7b2c\u4e00\u6b21\u5904\u7406\u8be5\u53d8\u91cf\u8be5\u5c42\uff0c\u521d\u59cb\u5316climatology\nelse:\nclimatology[f\"{var}_{level}\"].append(clim_yearly) # \u5c06\u8be5\u5e74\u4efd\u8be5\u5c42\u7684climatology\u6dfb\u52a0\u5230\u6570\u7ec4\nassert HOURS_PER_YEAR % num_shards_per_year == 0  # \u786e\u8ba4HOURS_PER_YEAR\uff08\u6bcf\u5e74\u7684\u5c0f\u65f6\u6570\uff09\u53ef\u4ee5\u88abnum_shards_per_year\uff08\u6bcf\u5e74\u7684\u5206\u7247\u6570\uff09\u6574\u9664\nnum_hrs_per_shard = HOURS_PER_YEAR // num_shards_per_year  # \u8ba1\u7b97\u6bcf\u4e2a\u5206\u7247\u5305\u542b\u591a\u5c11\u5c0f\u65f6\nfor shard_id in range(num_shards_per_year):  # \u5bf9\u4e8e\u6bcf\u4e2a\u5206\u7247\uff0c\u5faa\u73af\u8fed\u4ee3\nstart_id = shard_id * num_hrs_per_shard  # \u8ba1\u7b97\u8be5\u5206\u7247\u7684\u8d77\u59cb\u65f6\u95f4\nend_id = start_id + num_hrs_per_shard  # \u8ba1\u7b97\u8be5\u5206\u7247\u7684\u7ed3\u675f\u65f6\u95f4\n# \u5c06\u6bcf\u4e2a\u53d8\u91cf\u7684\u503c\u5212\u5206\u5230\u8be5\u5206\u7247\nsharded_data = {k: np_vars[k][start_id:end_id] for k in np_vars.keys()}\nnp.savez(\nos.path.join(save_dir, partition, f\"{year}_{shard_id}.npz\"),  # \u5c06\u6570\u636e\u4fdd\u5b58\u5230\u7279\u5b9a\u6587\u4ef6\u540d\u7684\u6587\u4ef6\u4e2d\n**sharded_data,  # \u4f7f\u7528\u5173\u952e\u5b57\u53c2\u6570\u5c06sharded_data\u4e2d\u7684\u6240\u6709\u9879\u4f20\u9012\u7ed9savez\u51fd\u6570\n)\nif partition == \"train\":  # \u5982\u679c\u5f53\u524d\u5206\u533a\u4e3a\u8bad\u7ec3\u5206\u533a\n# \u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\uff0c\u5982\u679c\u53d8\u91cf\u4e0d\u5728constant_fields\u5217\u8868\u4e2d\uff0c\u5219\u5bf9normalize_mean\u548cnormalize_std\u8fdb\u884c\u5806\u53e0\nfor var in normalize_mean.keys():\nif var not in constant_fields:\nnormalize_mean[var] = np.stack(normalize_mean[var], axis=0)\nnormalize_std[var] = np.stack(normalize_std[var], axis=0)\n# \u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\uff0c\u5bf9\u591a\u5e74\u7684\u503c\u8fdb\u884c\u805a\u5408\nfor var in normalize_mean.keys():\nif var not in constant_fields:\nmean, std = normalize_mean[var], normalize_std[var]\n# var(X) = E[var(X|Y)] + var(E[X|Y])\nvariance = (std**2).mean(axis=0) + (mean**2).mean(axis=0) - mean.mean(axis=0) ** 2\nstd = np.sqrt(variance)\n# E[X] = E[E[X|Y]]\nmean = mean.mean(axis=0)\nnormalize_mean[var] = mean\nnormalize_std[var] = std\n# \u5c06normalize_mean\u548cnormalize_std\u4fdd\u5b58\u5230\u6587\u4ef6\u4e2d\nnp.savez(os.path.join(save_dir, \"normalize_mean.npz\"), **normalize_mean)\nnp.savez(os.path.join(save_dir, \"normalize_std.npz\"), **normalize_std)\n# \u5bf9\u4e8e\u6240\u6709\u53d8\u91cf\uff0c\u8ba1\u7b97\u6c14\u5019\u5b66\uff08climatology\uff09\u503c\nfor var in climatology.keys():\nclimatology[var] = np.stack(climatology[var], axis=0)\nclimatology = {k: np.mean(v, axis=0) for k, v in climatology.items()}\n# \u5c06climatology\u4fdd\u5b58\u5230\u6587\u4ef6\u4e2d\nnp.savez(\nos.path.join(save_dir, partition, \"climatology.npz\"),\n**climatology,\n)\n</code></pre>"},{"location":"structure/","title":"Structure","text":""},{"location":"structure/#_1","title":"\u4ee3\u7801\u7ed3\u6784","text":"<pre><code>climax\n    global_forecast # \u5168\u7403\u9884\u62a5\n        __init__.py\n        datamodule.py\n        module.py\n        train.py\n    pretrain # \u9884\u8bad\u7ec3\n        __init__.py\n        datamodule.py\n        module.py\n        train.py\n        dataset.py\n    regional_forecast # \u533a\u57df\u9884\u62a5\n        __init__.py\n        datamodule.py\n        module.py\n        train.py\n        arch.py\n    utils # \u4e00\u4e9b\u51fd\u6570\n        data_utils.py\n        lr_scheduler.py\n        metrics.py\n        pos_embed.py\n    arch.py # \u603b\u4f53\u6846\u67b6\n    __init__.py\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#pretraining","title":"Pretraining","text":""},{"location":"usage/#data-preparation","title":"Data Preparation","text":"<p>The code for downloading and preprocessing CMIP6 data is coming soon</p>"},{"location":"usage/#training","title":"Training","text":"<p><pre><code>python src/climax/pretrain/train.py --config &lt;path/to/config&gt;\n</code></pre> For example, to pretrain ClimaX on MPI-ESM dataset on 8 GPUs use <pre><code>python src/climax/pretrain/train.py --config configs/pretrain_climax.yaml \\\n--trainer.strategy=ddp --trainer.devices=8 \\\n--trainer.max_epochs=100 \\\n--data.batch_size=16 \\\n--model.lr=5e-4 --model.beta_1=\"0.9\" --model.beta_2=\"0.95\" \\\n--model.weight_decay=1e-5\n</code></pre></p> <p>Tip</p> <p>Make sure to update the paths of the data directories in the config files (or override them via the CLI).</p>"},{"location":"usage/#pretrained-checkpoints","title":"Pretrained checkpoints","text":"<p>We provide two pretrained checkpoints, one was pretrained on 5.625deg data, and the other was pretrained on 1.40625deg data. Both checkpoints were pretrained using all 5 CMIP6 datasets.</p> <p>Usage: We can load the checkpoint by passing the checkpoint url to the training script. See below for examples.</p>"},{"location":"usage/#global-forecasting","title":"Global Forecasting","text":""},{"location":"usage/#data-preparation_1","title":"Data Preparation","text":"<p>First, download ERA5 data from WeatherBench. The data directory should look like the following <pre><code>5.625deg\n   |-- 10m_u_component_of_wind\n   |-- 10m_v_component_of_wind\n   |-- 2m_temperature\n   |-- constants.nc\n   |-- geopotential\n   |-- relative_humidity\n   |-- specific_humidity\n   |-- temperature\n   |-- toa_incident_solar_radiation\n   |-- total_precipitation\n   |-- u_component_of_wind\n   |-- v_component_of_wind\n</code></pre></p> <p>Then, preprocess the netcdf data into small numpy files and compute important statistics <pre><code>python src/data_preprocessing/nc2np_equally_era5.py \\\n--root_dir /mnt/data/5.625deg \\\n--save_dir /mnt/data/5.625deg_npz \\\n--start_train_year 1979 --start_val_year 2016 \\\n--start_test_year 2017 --end_year 2019 --num_shards 8\n</code></pre></p> <p>The preprocessed data directory will look like the following <pre><code>5.625deg_npz\n   |-- train\n   |-- val\n   |-- test\n   |-- normalize_mean.npz\n   |-- normalize_std.npz\n   |-- lat.npy\n   |-- lon.npy\n</code></pre></p>"},{"location":"usage/#training_1","title":"Training","text":"<p>To finetune ClimaX for global forecasting, use <pre><code>python src/climax/global_forecast/train.py --config &lt;path/to/config&gt;\n</code></pre> For example, to finetune ClimaX on 8 GPUs use <pre><code>python src/climax/global_forecast/train.py --config configs/global_forecast_climax.yaml \\\n--trainer.strategy=ddp --trainer.devices=8 \\\n--trainer.max_epochs=50 \\\n--data.root_dir=/mnt/data/5.625deg_npz \\\n--data.predict_range=72 --data.out_variables=['z_500','t_850','t2m'] \\\n--data.batch_size=16 \\\n--model.pretrained_path='https://climaxrelease.blob.core.windows.net/checkpoints/ClimaX-5.625deg.ckpt' \\\n--model.lr=5e-7 --model.beta_1=\"0.9\" --model.beta_2=\"0.99\" \\\n--model.weight_decay=1e-5\n</code></pre> To train ClimaX from scratch, set <code>--model.pretrained_path=\"\"</code>.</p>"},{"location":"usage/#regional-forecasting","title":"Regional Forecasting","text":""},{"location":"usage/#data-preparation_2","title":"Data Preparation","text":"<p>We use the same ERA5 data as in global forecasting and extract the regional data on the fly during training. If you have already downloaded and preprocessed the data, you do not have to do it again.</p>"},{"location":"usage/#training_2","title":"Training","text":"<p>To finetune ClimaX for regional forecasting, use <pre><code>python src/climax/regional_forecast/train.py --config &lt;path/to/config&gt;\n</code></pre> For example, to finetune ClimaX on North America using 8 GPUs, use <pre><code>python src/climax/regional_forecast/train.py --config configs/regional_forecast_climax.yaml \\\n--trainer.strategy=ddp --trainer.devices=8 \\\n--trainer.max_epochs=50 \\\n--data.root_dir=/mnt/data/5.625deg_npz \\\n--data.region=\"NorthAmerica\"\n--data.predict_range=72 --data.out_variables=['z_500','t_850','t2m'] \\\n--data.batch_size=16 \\\n--model.pretrained_path='https://climaxrelease.blob.core.windows.net/checkpoints/ClimaX-5.625deg.ckpt' \\\n--model.lr=5e-7 --model.beta_1=\"0.9\" --model.beta_2=\"0.99\" \\\n--model.weight_decay=1e-5\n</code></pre> To train ClimaX from scratch, set <code>--model.pretrained_path=\"\"</code>.</p>"},{"location":"usage/#visualization","title":"Visualization","text":"<p>Coming soon</p>"},{"location":"reference/global_forecast/","title":"Global Forecasting","text":""},{"location":"reference/global_forecast/#climax.global_forecast.module.GlobalForecastModule","title":"<code>GlobalForecastModule</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>PyTorch module for global forecasting with the ClimaX model.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>ClimaX</code> <p>ClimaX model.</p> required <code>pretrained_path</code> <code>str</code> <p>Path to pre-trained checkpoint.</p> <code>''</code> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>beta_1</code> <code>float</code> <p>Beta 1 for AdamW.</p> <code>0.9</code> <code>beta_2</code> <code>float</code> <p>Beta 2 for AdamW.</p> <code>0.99</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for AdamW.</p> <code>1e-05</code> <code>warmup_epochs</code> <code>int</code> <p>Number of warmup epochs.</p> <code>10000</code> <code>max_epochs</code> <code>int</code> <p>Number of total epochs.</p> <code>200000</code> <code>warmup_start_lr</code> <code>float</code> <p>Starting learning rate for warmup.</p> <code>1e-08</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-08</code> Source code in <code>src/climax/global_forecast/module.py</code> <pre><code>class GlobalForecastModule(nn.Module):\n\"\"\"PyTorch module for global forecasting with the ClimaX model.\n    Args:\n        net (ClimaX): ClimaX model.\n        pretrained_path (str, optional): Path to pre-trained checkpoint.\n        lr (float, optional): Learning rate.\n        beta_1 (float, optional): Beta 1 for AdamW.\n        beta_2 (float, optional): Beta 2 for AdamW.\n        weight_decay (float, optional): Weight decay for AdamW.\n        warmup_epochs (int, optional): Number of warmup epochs.\n        max_epochs (int, optional): Number of total epochs.\n        warmup_start_lr (float, optional): Starting learning rate for warmup.\n        eta_min (float, optional): Minimum learning rate.\n    \"\"\"\ndef __init__(\nself,\ndefault_vars: list,\npretrained_path: str = \"\",\nlr: float = 5e-4,\nbeta_1: float = 0.9,\nbeta_2: float = 0.99,\nweight_decay: float = 1e-5,\nwarmup_epochs: int = 10000,\nmax_epochs: int = 200000,\nwarmup_start_lr: float = 1e-8,\neta_min: float = 1e-8,\n):\nsuper().__init__()\nself.net=ClimaX(default_vars)\nself.lr=lr\nself.beta_1=beta_1\nself.beta_2=beta_2\nself.weight_decay=weight_decay\nself.warmup_epochs=warmup_epochs\nself.max_epochs=max_epochs\nself.warmup_start_lr=warmup_start_lr\nself.eta_min=eta_min\nif len(pretrained_path) &gt; 0:\nself.load_pretrained_weights(pretrained_path)\ndef load_pretrained_weights(self, pretrained_path):\nif pretrained_path.startswith('http'):\ncheckpoint = torch.hub.load_state_dict_from_url(pretrained_path)\nelse:\ncheckpoint = torch.load(pretrained_path, map_location=torch.device(\"cpu\"))\nprint(\"Loading pre-trained checkpoint from: %s\" % pretrained_path)\ncheckpoint_model = checkpoint[\"state_dict\"]\n# interpolate positional embedding\ninterpolate_pos_embed(self.net, checkpoint_model, new_size=self.net.img_size)\nstate_dict = self.state_dict()\n# logging.info(checkpoint_model.keys())\n# if self.net.parallel_patch_embed:\n#     if \"net.token_embeds.proj_weights\" not in checkpoint_model.keys():\n#         raise ValueError(\n#             \"Pretrained checkpoint does not have token_embeds.proj_weights for parallel processing. Please convert the checkpoints first or disable parallel patch_embed tokenization.\"\n#         )\nfor k in list(checkpoint_model.keys()):\nif \"channel\" in k:\ncheckpoint_model[k.replace(\"channel\", \"var\")] = checkpoint_model[k]\ndel checkpoint_model[k]\nfor k in list(checkpoint_model.keys()):\nif k not in state_dict.keys() or checkpoint_model[k].shape != state_dict[k].shape:\nprint(f\"Removing key {k} from pretrained checkpoint\")\ndel checkpoint_model[k]\n# load pre-trained model\nmsg = self.load_state_dict(checkpoint_model, strict=False)\nprint(msg)\ndef set_denormalization(self, mean, std):\nself.denormalization = transforms.Normalize(mean, std)\ndef set_lat_lon(self, lat, lon):\nself.lat = lat\nself.lon = lon\ndef set_pred_range(self, r):\nself.pred_range = r\ndef set_val_clim(self, clim):\nself.val_clim = clim\ndef set_test_clim(self, clim):\nself.test_clim = clim\ndef training_step(self, batch: Any):\nx, y, lead_times, variables, out_variables = batch\nloss_dict, _ = self.net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=self.lat)\nloss_dict = loss_dict[0] # type: ignore\nloss = loss_dict[\"loss\"]\nreturn loss\ndef validation_step(self, batch: Any):\nx, y, lead_times, variables, out_variables = batch\nif self.pred_range &lt; 24:\nlog_postfix = f\"{self.pred_range}_hours\"\nelse:\ndays = int(self.pred_range / 24)\nlog_postfix = f\"{days}_days\"\nloss, preds = self.net.forward(\nx,\ny,\nlead_times,\nvariables,\nout_variables,\nmetric=None,\nlat=self.lat,\n)\ntransform=self.denormalization\nclim=self.val_clim\nlog_postfix=log_postfix\nmetrics=[lat_weighted_mse_val, lat_weighted_rmse, lat_weighted_acc]\nall_loss_dicts = [m(preds, y, transform, out_variables, self.lat, clim, log_postfix) for m in metrics]\nloss_dict = {}\nfor d in all_loss_dicts:\nfor k in d.keys():\nloss_dict[k] = d[k]\nreturn loss_dict\ndef test_step(self, batch: Any):\nx, y, lead_times, variables, out_variables = batch\nif self.pred_range &lt; 24:\nlog_postfix = f\"{self.pred_range}_hours\"\nelse:\ndays = int(self.pred_range / 24)\nlog_postfix = f\"{days}_days\"\nloss, preds = self.net.forward(\nx,\ny,\nlead_times,\nvariables,\nout_variables,\nmetric=None,\nlat=self.lat,\n)\ntransform=self.denormalization\nclim=self.val_clim\nlog_postfix=log_postfix\nmetrics=[lat_weighted_mse_val, lat_weighted_rmse, lat_weighted_acc]\nall_loss_dicts = [m(preds, y, transform, out_variables, self.lat, clim, log_postfix) for m in metrics]\nloss_dict = {}\nfor d in all_loss_dicts:\nfor k in d.keys():\nloss_dict[k] = d[k]\nreturn preds , loss_dict\ndef configure_optimizers(self):\ndecay = []\nno_decay = []\nfor name, m in self.named_parameters():\nif \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\nno_decay.append(m)\nelse:\ndecay.append(m)\noptimizer = torch.optim.AdamW(\n[\n{\n\"params\": decay,\n\"lr\": self.lr,\n\"betas\": (self.beta_1, self.beta_2),\n\"weight_decay\": self.weight_decay,\n},\n{\n\"params\": no_decay,\n\"lr\": self.lr,\n\"betas\": (self.beta_1, self.beta_2),\n\"weight_decay\": 0,\n},\n]\n)\nlr_scheduler = LinearWarmupCosineAnnealingLR(\noptimizer,\nself.warmup_epochs,\nself.max_epochs,\nself.warmup_start_lr,\nself.eta_min,\n)\nreturn optimizer, lr_scheduler\n</code></pre>"},{"location":"reference/pretrain/","title":"Pretraining","text":""},{"location":"reference/pretrain/#climax.pretrain.datamodule.MultiSourceDataModule","title":"<code>MultiSourceDataModule</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>DataModule for multi-source data.</p> <p>Parameters:</p> Name Type Description Default <code>dict_root_dirs</code> <code>Dict</code> <p>Dictionary of root directories for each source.</p> required <code>dict_start_idx</code> <code>Dict</code> <p>Dictionary of start indices ratio (between 0.0 and 1.0) for each source.</p> required <code>dict_end_idx</code> <code>Dict</code> <p>Dictionary of end indices ratio (between 0.0 and 1.0) for each source.</p> required <code>dict_buffer_sizes</code> <code>Dict</code> <p>Dictionary of buffer sizes for each source.</p> required <code>dict_in_variables</code> <code>Dict</code> <p>Dictionary of input variables for each source.</p> required <code>dict_out_variables</code> <code>Dict</code> <p>Dictionary of output variables for each source.</p> required <code>dict_max_predict_ranges</code> <code>Dict</code> <p>Dictionary of maximum predict ranges for each source.</p> <code>{'mpi-esm': 28}</code> <code>dict_random_lead_time</code> <code>Dict</code> <p>Dictionary of whether to use random lead time for each source.</p> <code>{'mpi-esm': True}</code> <code>dict_hrs_each_step</code> <code>Dict</code> <p>Dictionary of hours each step for each source.</p> <code>{'mpi-esm': 6}</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>64</code> <code>num_workers</code> <code>int</code> <p>Number of workers.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin memory.</p> <code>False</code> Source code in <code>src/climax/pretrain/datamodule.py</code> <pre><code>class MultiSourceDataModule(LightningDataModule):\n\"\"\"DataModule for multi-source data.\n    Args:\n        dict_root_dirs (Dict): Dictionary of root directories for each source.\n        dict_start_idx (Dict): Dictionary of start indices ratio (between 0.0 and 1.0) for each source.\n        dict_end_idx (Dict): Dictionary of end indices ratio (between 0.0 and 1.0) for each source.\n        dict_buffer_sizes (Dict): Dictionary of buffer sizes for each source.\n        dict_in_variables (Dict): Dictionary of input variables for each source.\n        dict_out_variables (Dict): Dictionary of output variables for each source.\n        dict_max_predict_ranges (Dict, optional): Dictionary of maximum predict ranges for each source.\n        dict_random_lead_time (Dict, optional): Dictionary of whether to use random lead time for each source.\n        dict_hrs_each_step (Dict, optional): Dictionary of hours each step for each source.\n        batch_size (int, optional): Batch size.\n        num_workers (int, optional): Number of workers.\n        pin_memory (bool, optional): Whether to pin memory.\n    \"\"\"\ndef __init__(\nself,\ndict_root_dirs: Dict,\ndict_start_idx: Dict,\ndict_end_idx: Dict,\ndict_buffer_sizes: Dict,\ndict_in_variables: Dict,\ndict_out_variables: Dict,\ndict_max_predict_ranges: Dict = {\"mpi-esm\": 28},\ndict_random_lead_time: Dict = {\"mpi-esm\": True},\ndict_hrs_each_step: Dict = {\"mpi-esm\": 6},\nbatch_size: int = 64,\nnum_workers: int = 0,\npin_memory: bool = False,\n):\nsuper().__init__()\nif num_workers &gt; 1:\nraise NotImplementedError(\n\"num_workers &gt; 1 is not supported yet. Performance will likely degrage too with larger num_workers.\"\n)\n# this line allows to access init params with 'self.hparams' attribute\nself.save_hyperparameters(logger=False)\nout_variables = {}\nfor k, list_out in dict_out_variables.items():\nif list_out is not None:\nout_variables[k] = list_out\nelse:\nout_variables[k] = dict_in_variables[k]\nself.hparams.dict_out_variables = out_variables\nself.dict_lister_trains = {\nk: list(dp.iter.FileLister(os.path.join(root_dir, \"train\"))) for k, root_dir in dict_root_dirs.items()\n}\nself.train_dataset_args = {\nk: {\n\"max_predict_range\": dict_max_predict_ranges[k],\n\"random_lead_time\": dict_random_lead_time[k],\n\"hrs_each_step\": dict_hrs_each_step[k],\n}\nfor k in dict_root_dirs.keys()\n}\nself.transforms = self.get_normalize()\nself.output_transforms = self.get_normalize(self.hparams.dict_out_variables)\nself.dict_data_train: Optional[Dict] = None\ndef get_normalize(self, dict_variables: Optional[Dict] = None):\nif dict_variables is None:\ndict_variables = self.hparams.dict_in_variables\ndict_transforms = {}\nfor k in dict_variables.keys():\nroot_dir = self.hparams.dict_root_dirs[k]\nvariables = dict_variables[k]\nnormalize_mean = dict(np.load(os.path.join(root_dir, \"normalize_mean.npz\")))\nmean = []\nfor var in variables:\nif var != \"total_precipitation\":\nmean.append(normalize_mean[var])\nelse:\nmean.append(np.array([0.0]))\nnormalize_mean = np.concatenate(mean)\nnormalize_std = dict(np.load(os.path.join(root_dir, \"normalize_std.npz\")))\nnormalize_std = np.concatenate([normalize_std[var] for var in variables])\ndict_transforms[k] = transforms.Normalize(normalize_mean, normalize_std)\nreturn dict_transforms\ndef get_lat_lon(self):\n# assume different data sources have the same lat and lon coverage\nlat = np.load(os.path.join(list(self.hparams.dict_root_dirs.values())[0], \"lat.npy\"))\nlon = np.load(os.path.join(list(self.hparams.dict_root_dirs.values())[0], \"lon.npy\"))\nreturn lat, lon\ndef setup(self, stage: Optional[str] = None):\n# load datasets only if they're not loaded already\nif not self.dict_data_train:\ndict_data_train = {}\nfor k in self.dict_lister_trains.keys():\nlister_train = self.dict_lister_trains[k]\nstart_idx = self.hparams.dict_start_idx[k]\nend_idx = self.hparams.dict_end_idx[k]\nvariables = self.hparams.dict_in_variables[k]\nout_variables = self.hparams.dict_out_variables[k]\nmax_predict_range = self.hparams.dict_max_predict_ranges[k]\nrandom_lead_time = self.hparams.dict_random_lead_time[k]\nhrs_each_step = self.hparams.dict_hrs_each_step[k]\ntransforms = self.transforms[k]\noutput_transforms = self.output_transforms[k]\nbuffer_size = self.hparams.dict_buffer_sizes[k]\ndict_data_train[k] = ShuffleIterableDataset(\nIndividualForecastDataIter(\nForecast(\nNpyReader(\nlister_train,\nstart_idx=start_idx,\nend_idx=end_idx,\nvariables=variables,\nout_variables=out_variables,\nshuffle=True,\nmulti_dataset_training=True,\n),\nmax_predict_range=max_predict_range,\nrandom_lead_time=random_lead_time,\nhrs_each_step=hrs_each_step,\n),\ntransforms,\noutput_transforms,\n),\nbuffer_size,\n)\nself.dict_data_train = dict_data_train\ndef train_dataloader(self):\nif not torch.distributed.is_initialized():\nraise NotImplementedError(\"Only support distributed training\")\nelse:\nnode_rank = int(os.environ[\"NODE_RANK\"])\n# assert that number of datasets is the same as number of nodes\nnum_nodes = os.environ.get(\"NODES\", None)\nif num_nodes is not None:\nnum_nodes = int(num_nodes)\nassert num_nodes == len(self.dict_data_train.keys())\nfor idx, k in enumerate(self.dict_data_train.keys()):\nif idx == node_rank:\ndata_train = self.dict_data_train[k]\nbreak\n# This assumes that the number of datapoints are going to be the same for all datasets\nreturn DataLoader(\ndata_train,\nbatch_size=self.hparams.batch_size,\ndrop_last=True,\nnum_workers=self.hparams.num_workers,\npin_memory=self.hparams.pin_memory,\ncollate_fn=collate_fn,\n)\n</code></pre>"},{"location":"reference/pretrain/#climax.pretrain.module.PretrainModule","title":"<code>PretrainModule</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>Lightning module for pretraining the ClimaX model.</p> <p>Parameters:</p> Name Type Description Default <code>net</code> <code>ClimaX</code> <p>ClimaX model.</p> required <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.0005</code> <code>beta_1</code> <code>float</code> <p>Beta 1 for AdamW.</p> <code>0.9</code> <code>beta_2</code> <code>float</code> <p>Beta 2 for AdamW.</p> <code>0.95</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for AdamW.</p> <code>1e-05</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps.</p> <code>10000</code> <code>max_steps</code> <code>int</code> <p>Number of total steps.</p> <code>200000</code> <code>warmup_start_lr</code> <code>float</code> <p>Starting learning rate for warmup.</p> <code>1e-08</code> <code>eta_min</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-08</code> Source code in <code>src/climax/pretrain/module.py</code> <pre><code>class PretrainModule(LightningModule):\n\"\"\"Lightning module for pretraining the ClimaX model.\n    Args:\n        net (ClimaX): ClimaX model.\n        lr (float, optional): Learning rate.\n        beta_1 (float, optional): Beta 1 for AdamW.\n        beta_2 (float, optional): Beta 2 for AdamW.\n        weight_decay (float, optional): Weight decay for AdamW.\n        warmup_steps (int, optional): Number of warmup steps.\n        max_steps (int, optional): Number of total steps.\n        warmup_start_lr (float, optional): Starting learning rate for warmup.\n        eta_min (float, optional): Minimum learning rate.\n    \"\"\"\ndef __init__(\nself,\nnet: ClimaX,\nlr: float = 5e-4,\nbeta_1: float = 0.9,\nbeta_2: float = 0.95,\nweight_decay: float = 1e-5,\nwarmup_steps: int = 10000,\nmax_steps: int = 200000,\nwarmup_start_lr: float = 1e-8,\neta_min: float = 1e-8,\n):\nsuper().__init__()\nself.save_hyperparameters(logger=False, ignore=[\"net\"])\nself.net = net\ndef set_lat_lon(self, lat, lon):\nself.lat = lat\nself.lon = lon\ndef training_step(self, batch: Any, batch_idx: int):\nx, y, lead_times, variables, out_variables = batch\nloss_dict, _ = self.net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=self.lat)\nloss_dict = loss_dict[0]\nfor var in loss_dict.keys():\nself.log(\n\"train/\" + var,\nloss_dict[var],\non_step=True,\non_epoch=False,\nprog_bar=True,\n)\nloss = loss_dict[\"loss\"]\nreturn loss\ndef configure_optimizers(self):\ndecay = []\nno_decay = []\nfor name, m in self.named_parameters():\nif \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\nno_decay.append(m)\nelse:\ndecay.append(m)\noptimizer = torch.optim.AdamW(\n[\n{\n\"params\": decay,\n\"lr\": self.hparams.lr,\n\"betas\": (self.hparams.beta_1, self.hparams.beta_2),\n\"weight_decay\": self.hparams.weight_decay,\n},\n{\n\"params\": no_decay,\n\"lr\": self.hparams.lr,\n\"betas\": (self.hparams.beta_1, self.hparams.beta_2),\n\"weight_decay\": 0,\n},\n]\n)\nlr_scheduler = LinearWarmupCosineAnnealingLR(\noptimizer,\nself.hparams.warmup_steps,\nself.hparams.max_steps,\nself.hparams.warmup_start_lr,\nself.hparams.eta_min,\n)\nscheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\nreturn {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n</code></pre>"}]}